# 웹페이지 HTML&JavaScript code crawling
## 1. 정상 사이트 / 피싱 사이트 URL list crawling
## 2. 정상 사이트 / 피싱 사이트의 코드 크롤링
#### 피싱 사이트는 크롤링이 막혀 있어 phishTank에서 제공하는 url을 받아왔습니다.
------------------------------------------------------------------------------------------------------------------------------------------

## 피싱 사이트 HTML&JavaScript code crawling

import requests
from bs4 import BeautifulSoup
import pandas as pd

# CSV 파일 경로 설정
input_csv_path = 'phishing_url.csv'
output_csv_path = 'unique_phishing_url.csv'  

# CSV 파일 읽기
df = pd.read_csv(input_csv_path)

# 중복 행 제거
df_unique = df.drop_duplicates()

# 결과를 CSV 파일로 저장
df_unique.to_csv(output_csv_path, index=False)

# CSV 파일에서 URL 목록을 읽어오기
input_csv_path = 'unique_phishing_url.csv'
urls_df = pd.read_csv(input_csv_path)
urls = urls_df.iloc[:, 0].tolist()  # 첫 번째 열의 데이터를 리스트로 변환
# 결과를 저장할 리스트 초기화
results = []

# 각 URL에 대해 HTML 및 JavaScript 코드 추출
for url in urls:
    try:
        response = requests.get(url, timeout=10)  # 타임아웃 설정
        response.raise_for_status()  # 상태 코드가 200이 아니면 예외 발생
        html_content = response.text

        # BeautifulSoup을 사용하여 HTML 파싱
        soup = BeautifulSoup(html_content, 'html.parser')
        html_code = soup.prettify()

        # HTML 코드에서 JavaScript 코드 추출
        scripts = soup.find_all('script')
        javascript_codes = [script.string for script in scripts if script.string]
        javascript_code = "\n".join(javascript_codes)

        # 결과 저장
        results.append({
            'URL': url,
            'HTML Code': html_code,
            'JavaScript Code': javascript_code,
            'Error': ''
        })

    except requests.RequestException as e:
        # 오류 발생 시 URL과 오류 메시지 기록
        results.append({
            'URL': url,
            'HTML Code': '',
            'JavaScript Code': '',
            'Error': str(e)
        })

# 결과를 DataFrame으로 변환
results_df = pd.DataFrame(results)

# 결과를 CSV 파일로 저장
output_csv_path = 'phishing_results.csv'
results_df.to_csv(output_csv_path, index=False)


phishing_results_df = pd.read_csv('phishing_results.csv')
phishing_results_df

phishing_results_df = pd.read_csv('phishing_results.csv')

# 'Error' 열에 텍스트가 있는 행의 수 계산
error_count = phishing_results_df[phishing_results_df['Error'].notna() & (phishing_results_df['Error'] != '')].shape[0]

print(f"'Error' 열에 텍스트가 있는 행의 수: {error_count}")

import pandas as pd

# CSV 파일 읽기
phishing_results_df = pd.read_csv('phishing_results.csv')

# 'Error' 열에 텍스트가 없는 행만 필터링
phishing_results_df_no_error = phishing_results_df[phishing_results_df['Error'].isna() | (phishing_results_df['Error'] == '')]

# 결과를 CSV 파일로 저장
output_csv_path = 'valid_phishing_results.csv'
phishing_results_df_no_error.to_csv(output_csv_path, index=False)

import pandas as pd

# CSV 파일 읽기
phishing_results_df = pd.read_csv('valid_phishing_results.csv')

# 'Error' 열 제거
final_phishing_df = phishing_results_df.drop(columns=['Error'])
final_phishing_df = final_phishing_df[:40]
# 결과를 CSV 파일로 저장
output_csv_path = 'final_phishing.csv'
final_phishing_df.to_csv(output_csv_path, index=False)

final_phishing_df

## 정상 사이트 HTML&JavaScript code crawling

import pandas as pd

# 기존 도메인 목록을 HTTPS 주소로 변환하고 앞에 "www." 추가한 리스트
url_list = [
    "https://www.youtube.com",
    "https://www.google.com",
    "https://www.naver.com",
    "https://www.coupang.com",
    "https://www.ravielink.xyz",
    "https://www.ilbe.com",
    "https://www.fmkorea.com",
    "https://www.tv40.wiki",
    "https://www.draplay.info",
    "https://www.daum.net",
    "https://www.kakao.com",
    "https://www.kr-weathernews.com",
    "https://www.arca.live",
    "https://www.twitter.com",
    "https://www.ruliweb.com",
    "https://www.tistory.com",
    "https://www.aliexpress.com",
    "https://www.linkkf.net",
    "https://www.11st.co.kr",
    "https://www.inven.co.kr",
    "https://www.facebook.com",
    "https://www.appier.net",
    "https://www.mmtcld.net",
    "https://www.nate.com",
    "https://www.gmarket.co.kr",
    "https://www.instagram.com",
    "https://www.donga.com",
    "https://www.ppomppu.co.kr",
    "https://www.dogdrip.net",
    "https://www.asianhd2.cc",
    "https://www.msn.com",
    "https://www.theqoo.net",
    "https://www.humoruniv.com",
    "https://www.nexon.com"
]

# 추가할 도메인 목록
additional_urls = [
    "https://www.sauceflex.com",
    "https://www.ssg.com",
    "https://www.google.co.kr",
    "https://www.adbrix.io",
    "https://www.etoland.co.kr",
    "https://www.buzzvil.com",
    "https://www.dhlottery.co.kr",
    "https://www.tdgall.com",
    "https://www.linkmine.co.kr",
    "https://www.oliveyoung.co.kr",
    "https://www.shinhancard.com",
    "https://www.kbcard.com",
    "https://www.afreecatv.com",
    "https://www.netflix.com",
    "https://www.chosun.com",
    "https://www.auction.co.kr",
    "https://www.temu.com",
    "https://www.kt.com",
    "https://www.openai.com",
    "https://www.newspic.kr",
    "https://www.3o3.co.kr",
    "https://www.gcltracker.com",
    "https://www.newtoki331.com",
    "https://www.blacktoon285.com",
    "https://www.bobaedream.co.kr",
    "https://www.cjonstyle.com",
    "https://www.mediacategory.com",
    "https://www.info21.khu.ac.kr/com/LoginCtr/login.do?sso=ok"
]

# 모든 도메인을 합친 리스트
url_list.extend(additional_urls)

# 데이터프레임으로 변환
df = pd.DataFrame(url_list, columns=["URL"])

# 중복 URL 제거
df_unique = df.drop_duplicates()

# 결과를 CSV 파일로 저장
output_csv_path = 'normal_url.csv'
df_unique.to_csv(output_csv_path, index=False)

# 최종 URL 개수 출력
final_count = df_unique.shape[0]
print(f"중복 제거된 최종 URL 개수: {final_count}")

# 중복 제거된 URL 목록 출력
print("중복 제거된 URL 목록:")
print(df_unique)


# CSV 파일에서 URL 목록을 읽어오기
input_csv_path = 'normal_url.csv'
urls_df = pd.read_csv(input_csv_path)
urls = urls_df.iloc[:, 0].tolist()  # 첫 번째 열의 데이터를 리스트로 변환
# 결과를 저장할 리스트 초기화
results = []

# 각 URL에 대해 HTML 및 JavaScript 코드 추출
for url in urls:
    try:
        response = requests.get(url, timeout=10)  # 타임아웃 설정
        response.raise_for_status()  # 상태 코드가 200이 아니면 예외 발생
        html_content = response.text

        # BeautifulSoup을 사용하여 HTML 파싱
        soup = BeautifulSoup(html_content, 'html.parser')
        html_code = soup.prettify()

        # HTML 코드에서 JavaScript 코드 추출
        scripts = soup.find_all('script')
        javascript_codes = [script.string for script in scripts if script.string]
        javascript_code = "\n".join(javascript_codes)

        # 결과 저장
        results.append({
            'URL': url,
            'HTML Code': html_code,
            'JavaScript Code': javascript_code,
            'Error': ''
        })

    except requests.RequestException as e:
        # 오류 발생 시 URL과 오류 메시지 기록
        results.append({
            'URL': url,
            'HTML Code': '',
            'JavaScript Code': '',
            'Error': str(e)
        })

# 결과를 DataFrame으로 변환
results_df = pd.DataFrame(results)

# 결과를 CSV 파일로 저장
output_csv_path = 'normal_results.csv'
results_df.to_csv(output_csv_path, index=False)

normal_results_df = pd.read_csv('normal_results.csv')
# 오류가 발생한 URL 수를 파악
error_count = normal_results_df[normal_results_df['Error'] != ''].shape[0]

print(f"오류가 발생한 URL의 수: {error_count}")
# 오류가 발생하지 않은 URL만 필터링하여 새로운 DataFrame 생성
normal_valid_results_df = normal_results_df[normal_results_df['Error'] == '']
# 'Error' 열을 삭제
final_normal_df = normal_results_df.drop(columns=['Error'])
# 결과를 CSV 파일로 저장
valid_output_csv_path = 'final_normal.csv'
final_normal_df.to_csv(valid_output_csv_path, index=False)
final_normal_df.describe()

import pandas as pd

# CSV 파일 읽기
input_csv_path = 'final_normal.csv'
df = pd.read_csv(input_csv_path)

# HTML과 JavaScript 코드가 비어있는 행 제거
df_filtered = df[~((df['HTML Code'].isna() | (df['HTML Code'] == '')) |
                   (df['JavaScript Code'].isna() | (df['JavaScript Code'] == '')))]
final_normal_df = df_filtered[:40]
# 결과를 CSV 파일로 저장
output_csv_path = 'final_normal.csv'
final_normal_df.to_csv(output_csv_path, index=False)
final_normal_df

### URL만 사용할 데이터 정상 웹사이트 500개 크롤링

from requests.exceptions import ConnectionError
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
import pandas as pd
import random

targets = [
    "https://www.youtube.com",
    "https://www.google.com",
    "https://www.naver.com",
    "https://www.daum.net",
    "https://www.arca.live",
    "https://www.twitter.com",
    "https://www.tistory.com",
    "https://www.aliexpress.com",
    "https://www.11st.co.kr",
    "https://www.inven.co.kr",
    "https://www.facebook.com",
    "https://www.nate.com",
    "https://www.gmarket.co.kr",
    "https://www.instagram.com",
    "https://www.donga.com",
    "https://www.ppomppu.co.kr",
    "https://www.asianhd2.cc",
    "https://www.msn.com",
    "https://www.theqoo.net",
    "https://www.nexon.com",
    "https://www.sauceflex.com",
    "https://www.ssg.com",
    "https://www.google.co.kr",
    "https://www.adbrix.io",
    "https://www.etoland.co.kr",
    "https://www.buzzvil.com",
    "https://www.dhlottery.co.kr",
    "https://www.oliveyoung.co.kr",
    "https://www.shinhancard.com",
    "https://www.kbcard.com",
    "https://www.afreecatv.com",
    "https://www.netflix.com",
    "https://www.chosun.com",
    "https://www.auction.co.kr",
    "https://www.temu.com",
    "https://www.kt.com",
    "https://www.newspic.kr"
]

max_pages_per_url = 25

def get_random_links(url, max_links=25):
    links = set()
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        anchor_tags = soup.find_all('a', href=True)
        random.shuffle(anchor_tags)
        for tag in anchor_tags:
            href = tag['href']
            full_url = requests.compat.urljoin(url, href)
            if full_url not in links and full_url.startswith('http'):
                links.add(full_url)
                if len(links) >= max_links:
                    break
    except (ConnectionError, requests.exceptions.RequestException):
        pass
    return list(links)

def collect_urls(targets, max_pages_per_url):
    collected_urls = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(get_random_links, target, max_pages_per_url): target for target in targets}
        for future in as_completed(futures):
            target = futures[future]
            try:
                links = future.result()
                collected_urls.extend(links)
                print(f"{target} - {len(links)} links collected.")
            except Exception as exc:
                print(f"{target} - exception occurred: {exc}")
    return collected_urls

if __name__ == '__main__':
    all_urls = collect_urls(targets, max_pages_per_url)
    df = pd.DataFrame(all_urls, columns=["URL"])
    df.to_csv('random_urls.csv', index=False)
    print("CSV file has been created with 1000 URLs.")


import pandas as pd
import random

# 기존 CSV 파일을 읽어들임
df = pd.read_csv('random_urls.csv')

# 500개의 URL을 무작위로 선택
sampled_df = df.sample(n=500, random_state=1)

# 선택된 500개의 URL을 새로운 CSV 파일로 저장
sampled_df.to_csv('sampled_random_urls.csv', index=False)

print("CSV 파일이 500개의 URL로 저장되었습니다.")


### URL만 사용할 데이터 피싱 웹사이트 500개는 openphish에서 최신 500개 데이터를 수집하였다.
